{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 70000 x 784 and label size 70000\n"
     ]
    }
   ],
   "source": [
    "# Use the notion of y=f(X)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print('Data size: {0} x {1} and label size {2}'.format(X.shape[0],X.shape[1],y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size=56000, test set size=14000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N = len(X)\n",
    "M = int(N*4/5)\n",
    "print(\"train set size={0}, test set size={1}\".format(M, N-M))\n",
    "shuffle_index = np.random.permutation(N)\n",
    "# reshuffle the data and use M samples as training and N-M as test\n",
    "X_train, X_test = X[shuffle_index[:M],:], X[shuffle_index[M:],:]\n",
    "y_train, y_test = y[shuffle_index[:M]], y[shuffle_index[M:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "LogisticRegression Confusion matrix: \n",
      "[[5394    1   18   12    9   47   36    9   34    8]\n",
      " [   1 6144   22   18    5   23    6   18   64   10]\n",
      " [  32   63 4948   97   63   29   70   72  140   24]\n",
      " [  20   18  134 5072    5  185   20   59  106   57]\n",
      " [  12   24   28   10 5084    6   55   19   52  192]\n",
      " [  73   21   40  198   56 4324  105   20  164   56]\n",
      " [  37   14   41    3   40   65 5262    6   30    2]\n",
      " [  20   28   73   24   58    7    3 5449   21  198]\n",
      " [  28   99   51  140   29  163   45   26 4781   70]\n",
      " [  29   36   17   68  149   33    2  158   51 5012]]\n",
      "LogisticRegression Classifier accuracy on the training set is 0.9191071428571429\n",
      "LogisticRegression Classifier accuracy on the test set is 0.9188571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('LogisticRegression')\n",
    "lr_clf = LogisticRegression(max_iter=50, random_state=42, multi_class='multinomial',solver='lbfgs')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_train_pred = cross_val_predict(lr_clf, X_train, y_train, cv=4)\n",
    "# Compute confusion matrix and accuracy score on the training set\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "print('LogisticRegression Confusion matrix: \\n{0}'.format(conf_mx))\n",
    "print('LogisticRegression Classifier accuracy on the training set is {0}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "# Compute accuracy on the test set\n",
    "y_pred_test = lr_clf.predict(X_test)\n",
    "print('LogisticRegression Classifier accuracy on the test set is {0}'.format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "Iteration 1, loss = 2.59616466\n",
      "Iteration 2, loss = 0.90413396\n",
      "Iteration 3, loss = 0.64608320\n",
      "Iteration 4, loss = 0.51734009\n",
      "Iteration 5, loss = 0.43955791\n",
      "Iteration 6, loss = 0.38813506\n",
      "Iteration 7, loss = 0.34934155\n",
      "Iteration 8, loss = 0.31744697\n",
      "Iteration 9, loss = 0.29395749\n",
      "Iteration 10, loss = 0.27409681\n",
      "Iteration 11, loss = 0.25711145\n",
      "Iteration 12, loss = 0.24248883\n",
      "Iteration 13, loss = 0.23044228\n",
      "Iteration 14, loss = 0.21976158\n",
      "Iteration 15, loss = 0.21064804\n",
      "Iteration 16, loss = 0.20212872\n",
      "Iteration 17, loss = 0.19440145\n",
      "Iteration 18, loss = 0.18840743\n",
      "Iteration 19, loss = 0.18266774\n",
      "Iteration 20, loss = 0.17633444\n",
      "Iteration 21, loss = 0.17145999\n",
      "Iteration 22, loss = 0.16596337\n",
      "Iteration 23, loss = 0.16138843\n",
      "Iteration 24, loss = 0.15717610\n",
      "Iteration 25, loss = 0.15358380\n",
      "Iteration 26, loss = 0.15007566\n",
      "Iteration 27, loss = 0.14655255\n",
      "Iteration 28, loss = 0.14307415\n",
      "Iteration 29, loss = 0.14001496\n",
      "Iteration 30, loss = 0.13679172\n",
      "Iteration 31, loss = 0.13350067\n",
      "Iteration 32, loss = 0.13136323\n",
      "Iteration 33, loss = 0.12821726\n",
      "Iteration 34, loss = 0.12579033\n",
      "Iteration 35, loss = 0.12341836\n",
      "Iteration 36, loss = 0.12127372\n",
      "Iteration 37, loss = 0.11877444\n",
      "Iteration 38, loss = 0.11677760\n",
      "Iteration 39, loss = 0.11429287\n",
      "Iteration 40, loss = 0.11307035\n",
      "Iteration 41, loss = 0.11105584\n",
      "Iteration 42, loss = 0.10892655\n",
      "Iteration 43, loss = 0.10727828\n",
      "Iteration 44, loss = 0.10555098\n",
      "Iteration 45, loss = 0.10361577\n",
      "Iteration 46, loss = 0.10234377\n",
      "Iteration 47, loss = 0.10101169\n",
      "Iteration 48, loss = 0.09936451\n",
      "Iteration 49, loss = 0.09754662\n",
      "Iteration 50, loss = 0.09621402\n",
      "Iteration 51, loss = 0.09494366\n",
      "Iteration 52, loss = 0.09376177\n",
      "Iteration 53, loss = 0.09220834\n",
      "Iteration 54, loss = 0.09081838\n",
      "Iteration 55, loss = 0.08970144\n",
      "Iteration 56, loss = 0.08877456\n",
      "Iteration 57, loss = 0.08739304\n",
      "Iteration 58, loss = 0.08625664\n",
      "Iteration 59, loss = 0.08490151\n",
      "Iteration 60, loss = 0.08404266\n",
      "Iteration 61, loss = 0.08271842\n",
      "Iteration 62, loss = 0.08189700\n",
      "Iteration 63, loss = 0.08083884\n",
      "Iteration 64, loss = 0.07950170\n",
      "Iteration 65, loss = 0.07872864\n",
      "Iteration 66, loss = 0.07778366\n",
      "Iteration 67, loss = 0.07704556\n",
      "Iteration 68, loss = 0.07605883\n",
      "Iteration 69, loss = 0.07526292\n",
      "Iteration 70, loss = 0.07400772\n",
      "Iteration 71, loss = 0.07348645\n",
      "Iteration 72, loss = 0.07259032\n",
      "Iteration 73, loss = 0.07165270\n",
      "Iteration 74, loss = 0.07069825\n",
      "Iteration 75, loss = 0.07029279\n",
      "Iteration 76, loss = 0.06947017\n",
      "Iteration 77, loss = 0.06843470\n",
      "Iteration 78, loss = 0.06756344\n",
      "Iteration 79, loss = 0.06692638\n",
      "Iteration 80, loss = 0.06628155\n",
      "Iteration 81, loss = 0.06544465\n",
      "Iteration 82, loss = 0.06467170\n",
      "Iteration 83, loss = 0.06395095\n",
      "Iteration 84, loss = 0.06311225\n",
      "Iteration 85, loss = 0.06251964\n",
      "Iteration 86, loss = 0.06213505\n",
      "Iteration 87, loss = 0.06150138\n",
      "Iteration 88, loss = 0.06075448\n",
      "Iteration 89, loss = 0.05997653\n",
      "Iteration 90, loss = 0.05932861\n",
      "Iteration 91, loss = 0.05895364\n",
      "Iteration 92, loss = 0.05815624\n",
      "Iteration 93, loss = 0.05738815\n",
      "Iteration 94, loss = 0.05671285\n",
      "Iteration 95, loss = 0.05624710\n",
      "Iteration 96, loss = 0.05559500\n",
      "Iteration 97, loss = 0.05500698\n",
      "Iteration 98, loss = 0.05461185\n",
      "Iteration 99, loss = 0.05410787\n",
      "Iteration 100, loss = 0.05355453\n",
      "Iteration 101, loss = 0.05301160\n",
      "Iteration 102, loss = 0.05222951\n",
      "Iteration 103, loss = 0.05174273\n",
      "Iteration 104, loss = 0.05134162\n",
      "Iteration 105, loss = 0.05090772\n",
      "Iteration 106, loss = 0.05050908\n",
      "Iteration 107, loss = 0.04963111\n",
      "Iteration 108, loss = 0.04953918\n",
      "Iteration 109, loss = 0.04884389\n",
      "Iteration 110, loss = 0.04854889\n",
      "Iteration 111, loss = 0.04778043\n",
      "Iteration 112, loss = 0.04730879\n",
      "Iteration 113, loss = 0.04694686\n",
      "Iteration 114, loss = 0.04683219\n",
      "Iteration 115, loss = 0.04616600\n",
      "Iteration 116, loss = 0.04578813\n",
      "Iteration 117, loss = 0.04526805\n",
      "Iteration 118, loss = 0.04475069\n",
      "Iteration 119, loss = 0.04454754\n",
      "Iteration 120, loss = 0.04391458\n",
      "Iteration 121, loss = 0.04373144\n",
      "Iteration 122, loss = 0.04304299\n",
      "Iteration 123, loss = 0.04276060\n",
      "Iteration 124, loss = 0.04229776\n",
      "Iteration 125, loss = 0.04182180\n",
      "Iteration 126, loss = 0.04171911\n",
      "Iteration 127, loss = 0.04117158\n",
      "Iteration 128, loss = 0.04084909\n",
      "Iteration 129, loss = 0.04054548\n",
      "Iteration 130, loss = 0.03999333\n",
      "Iteration 131, loss = 0.03958501\n",
      "Iteration 132, loss = 0.03929307\n",
      "Iteration 133, loss = 0.03888775\n",
      "Iteration 134, loss = 0.03870426\n",
      "Iteration 135, loss = 0.03831037\n",
      "Iteration 136, loss = 0.03786614\n",
      "Iteration 137, loss = 0.03750425\n",
      "Iteration 138, loss = 0.03725237\n",
      "Iteration 139, loss = 0.03704823\n",
      "Iteration 140, loss = 0.03662495\n",
      "Iteration 141, loss = 0.03600500\n",
      "Iteration 142, loss = 0.03586256\n",
      "Iteration 143, loss = 0.03545078\n",
      "Iteration 144, loss = 0.03525128\n",
      "Iteration 145, loss = 0.03483396\n",
      "Iteration 146, loss = 0.03460856\n",
      "Iteration 147, loss = 0.03429744\n",
      "Iteration 148, loss = 0.03395375\n",
      "Iteration 149, loss = 0.03350742\n",
      "Iteration 150, loss = 0.03312015\n",
      "Iteration 151, loss = 0.03305872\n",
      "Iteration 152, loss = 0.03269909\n",
      "Iteration 153, loss = 0.03235283\n",
      "Iteration 154, loss = 0.03221144\n",
      "Iteration 155, loss = 0.03171122\n",
      "Iteration 156, loss = 0.03176553\n",
      "Iteration 157, loss = 0.03114920\n",
      "Iteration 158, loss = 0.03084346\n",
      "Iteration 159, loss = 0.03092391\n",
      "Iteration 160, loss = 0.03048745\n",
      "Iteration 161, loss = 0.03021080\n",
      "Iteration 162, loss = 0.02982457\n",
      "Iteration 163, loss = 0.02973551\n",
      "Iteration 164, loss = 0.02943039\n",
      "Iteration 165, loss = 0.02918454\n",
      "Iteration 166, loss = 0.02904421\n",
      "Iteration 167, loss = 0.02856895\n",
      "Iteration 168, loss = 0.02856652\n",
      "Iteration 169, loss = 0.02840830\n",
      "Iteration 170, loss = 0.02821915\n",
      "Iteration 171, loss = 0.02775516\n",
      "Iteration 172, loss = 0.02759013\n",
      "Iteration 173, loss = 0.02728905\n",
      "Iteration 174, loss = 0.02711534\n",
      "Iteration 175, loss = 0.02680917\n",
      "Iteration 176, loss = 0.02669561\n",
      "Iteration 177, loss = 0.02650459\n",
      "Iteration 178, loss = 0.02630164\n",
      "Iteration 179, loss = 0.02601407\n",
      "Iteration 180, loss = 0.02567444\n",
      "Iteration 181, loss = 0.02562549\n",
      "Iteration 182, loss = 0.02528181\n",
      "Iteration 183, loss = 0.02527301\n",
      "Iteration 184, loss = 0.02488806\n",
      "Iteration 185, loss = 0.02473604\n",
      "Iteration 186, loss = 0.02457214\n",
      "Iteration 187, loss = 0.02421246\n",
      "Iteration 188, loss = 0.02411707\n",
      "Iteration 189, loss = 0.02395895\n",
      "Iteration 190, loss = 0.02366235\n",
      "Iteration 191, loss = 0.02351033\n",
      "Iteration 192, loss = 0.02338823\n",
      "Iteration 193, loss = 0.02322673\n",
      "Iteration 194, loss = 0.02303303\n",
      "Iteration 195, loss = 0.02283576\n",
      "Iteration 196, loss = 0.02268449\n",
      "Iteration 197, loss = 0.02241474\n",
      "Iteration 198, loss = 0.02234662\n",
      "Iteration 199, loss = 0.02210912\n",
      "Iteration 200, loss = 0.02194648\n",
      "Iteration 201, loss = 0.02174910\n",
      "Iteration 202, loss = 0.02152229\n",
      "Iteration 203, loss = 0.02135778\n",
      "Iteration 204, loss = 0.02119716\n",
      "Iteration 205, loss = 0.02102361\n",
      "Iteration 206, loss = 0.02098133\n",
      "Iteration 207, loss = 0.02072101\n",
      "Iteration 208, loss = 0.02046686\n",
      "Iteration 209, loss = 0.02044168\n",
      "Iteration 210, loss = 0.02031516\n",
      "Iteration 211, loss = 0.02018133\n",
      "Iteration 212, loss = 0.01997165\n",
      "Iteration 213, loss = 0.01977526\n",
      "Iteration 214, loss = 0.01954491\n",
      "Iteration 215, loss = 0.01947761\n",
      "Iteration 216, loss = 0.01925860\n",
      "Iteration 217, loss = 0.01913535\n",
      "Iteration 218, loss = 0.01895116\n",
      "Iteration 219, loss = 0.01882121\n",
      "Iteration 220, loss = 0.01880313\n",
      "Iteration 221, loss = 0.01837497\n",
      "Iteration 222, loss = 0.01852490\n",
      "Iteration 223, loss = 0.01829778\n",
      "Iteration 224, loss = 0.01821774\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.08869725\n",
      "Iteration 2, loss = 1.09826933\n",
      "Iteration 3, loss = 0.77599466\n",
      "Iteration 4, loss = 0.62158176\n",
      "Iteration 5, loss = 0.51817286\n",
      "Iteration 6, loss = 0.44811714\n",
      "Iteration 7, loss = 0.39692544\n",
      "Iteration 8, loss = 0.35608620\n",
      "Iteration 9, loss = 0.32330154\n",
      "Iteration 10, loss = 0.29738598\n",
      "Iteration 11, loss = 0.27588586\n",
      "Iteration 12, loss = 0.25840575\n",
      "Iteration 13, loss = 0.24116914\n",
      "Iteration 14, loss = 0.22773987\n",
      "Iteration 15, loss = 0.21597094\n",
      "Iteration 16, loss = 0.20516339\n",
      "Iteration 17, loss = 0.19557251\n",
      "Iteration 18, loss = 0.18744263\n",
      "Iteration 19, loss = 0.17891337\n",
      "Iteration 20, loss = 0.17261627\n",
      "Iteration 21, loss = 0.16595041\n",
      "Iteration 22, loss = 0.15982027\n",
      "Iteration 23, loss = 0.15408192\n",
      "Iteration 24, loss = 0.14918429\n",
      "Iteration 25, loss = 0.14425180\n",
      "Iteration 26, loss = 0.14004446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.13577298\n",
      "Iteration 28, loss = 0.13231190\n",
      "Iteration 29, loss = 0.12884851\n",
      "Iteration 30, loss = 0.12509071\n",
      "Iteration 31, loss = 0.12205101\n",
      "Iteration 32, loss = 0.11863080\n",
      "Iteration 33, loss = 0.11575994\n",
      "Iteration 34, loss = 0.11318374\n",
      "Iteration 35, loss = 0.11090415\n",
      "Iteration 36, loss = 0.10797565\n",
      "Iteration 37, loss = 0.10566418\n",
      "Iteration 38, loss = 0.10356656\n",
      "Iteration 39, loss = 0.10137630\n",
      "Iteration 40, loss = 0.09910441\n",
      "Iteration 41, loss = 0.09661711\n",
      "Iteration 42, loss = 0.09501602\n",
      "Iteration 43, loss = 0.09332739\n",
      "Iteration 44, loss = 0.09152188\n",
      "Iteration 45, loss = 0.09005016\n",
      "Iteration 46, loss = 0.08764279\n",
      "Iteration 47, loss = 0.08630291\n",
      "Iteration 48, loss = 0.08461030\n",
      "Iteration 49, loss = 0.08309527\n",
      "Iteration 50, loss = 0.08193763\n",
      "Iteration 51, loss = 0.08044364\n",
      "Iteration 52, loss = 0.07891886\n",
      "Iteration 53, loss = 0.07766530\n",
      "Iteration 54, loss = 0.07631685\n",
      "Iteration 55, loss = 0.07506704\n",
      "Iteration 56, loss = 0.07388748\n",
      "Iteration 57, loss = 0.07250211\n",
      "Iteration 58, loss = 0.07159542\n",
      "Iteration 59, loss = 0.07045705\n",
      "Iteration 60, loss = 0.06927702\n",
      "Iteration 61, loss = 0.06804343\n",
      "Iteration 62, loss = 0.06704806\n",
      "Iteration 63, loss = 0.06604297\n",
      "Iteration 64, loss = 0.06489521\n",
      "Iteration 65, loss = 0.06395876\n",
      "Iteration 66, loss = 0.06302007\n",
      "Iteration 67, loss = 0.06201244\n",
      "Iteration 68, loss = 0.06108915\n",
      "Iteration 69, loss = 0.06036506\n",
      "Iteration 70, loss = 0.05930018\n",
      "Iteration 71, loss = 0.05843748\n",
      "Iteration 72, loss = 0.05753867\n",
      "Iteration 73, loss = 0.05671366\n",
      "Iteration 74, loss = 0.05607117\n",
      "Iteration 75, loss = 0.05557174\n",
      "Iteration 76, loss = 0.05460170\n",
      "Iteration 77, loss = 0.05368898\n",
      "Iteration 78, loss = 0.05317863\n",
      "Iteration 79, loss = 0.05218081\n",
      "Iteration 80, loss = 0.05178292\n",
      "Iteration 81, loss = 0.05094009\n",
      "Iteration 82, loss = 0.05021007\n",
      "Iteration 83, loss = 0.04946704\n",
      "Iteration 84, loss = 0.04922417\n",
      "Iteration 85, loss = 0.04845453\n",
      "Iteration 86, loss = 0.04787050\n",
      "Iteration 87, loss = 0.04725301\n",
      "Iteration 88, loss = 0.04637227\n",
      "Iteration 89, loss = 0.04572700\n",
      "Iteration 90, loss = 0.04517352\n",
      "Iteration 91, loss = 0.04466661\n",
      "Iteration 92, loss = 0.04401298\n",
      "Iteration 93, loss = 0.04344726\n",
      "Iteration 94, loss = 0.04284020\n",
      "Iteration 95, loss = 0.04278479\n",
      "Iteration 96, loss = 0.04188962\n",
      "Iteration 97, loss = 0.04146811\n",
      "Iteration 98, loss = 0.04078634\n",
      "Iteration 99, loss = 0.04013422\n",
      "Iteration 100, loss = 0.03977747\n",
      "Iteration 101, loss = 0.03946437\n",
      "Iteration 102, loss = 0.03878435\n",
      "Iteration 103, loss = 0.03834898\n",
      "Iteration 104, loss = 0.03794542\n",
      "Iteration 105, loss = 0.03738776\n",
      "Iteration 106, loss = 0.03695903\n",
      "Iteration 107, loss = 0.03652961\n",
      "Iteration 108, loss = 0.03588097\n",
      "Iteration 109, loss = 0.03570690\n",
      "Iteration 110, loss = 0.03524472\n",
      "Iteration 111, loss = 0.03472554\n",
      "Iteration 112, loss = 0.03426072\n",
      "Iteration 113, loss = 0.03381620\n",
      "Iteration 114, loss = 0.03348524\n",
      "Iteration 115, loss = 0.03303909\n",
      "Iteration 116, loss = 0.03259791\n",
      "Iteration 117, loss = 0.03228256\n",
      "Iteration 118, loss = 0.03182654\n",
      "Iteration 119, loss = 0.03150895\n",
      "Iteration 120, loss = 0.03116347\n",
      "Iteration 121, loss = 0.03074518\n",
      "Iteration 122, loss = 0.03035371\n",
      "Iteration 123, loss = 0.03012002\n",
      "Iteration 124, loss = 0.02959858\n",
      "Iteration 125, loss = 0.02915303\n",
      "Iteration 126, loss = 0.02901116\n",
      "Iteration 127, loss = 0.02866468\n",
      "Iteration 128, loss = 0.02828280\n",
      "Iteration 129, loss = 0.02787465\n",
      "Iteration 130, loss = 0.02770049\n",
      "Iteration 131, loss = 0.02730570\n",
      "Iteration 132, loss = 0.02701050\n",
      "Iteration 133, loss = 0.02669501\n",
      "Iteration 134, loss = 0.02647273\n",
      "Iteration 135, loss = 0.02603862\n",
      "Iteration 136, loss = 0.02576260\n",
      "Iteration 137, loss = 0.02549586\n",
      "Iteration 138, loss = 0.02526221\n",
      "Iteration 139, loss = 0.02502192\n",
      "Iteration 140, loss = 0.02468179\n",
      "Iteration 141, loss = 0.02439306\n",
      "Iteration 142, loss = 0.02396074\n",
      "Iteration 143, loss = 0.02393033\n",
      "Iteration 144, loss = 0.02361383\n",
      "Iteration 145, loss = 0.02328231\n",
      "Iteration 146, loss = 0.02314331\n",
      "Iteration 147, loss = 0.02262416\n",
      "Iteration 148, loss = 0.02247246\n",
      "Iteration 149, loss = 0.02231901\n",
      "Iteration 150, loss = 0.02208468\n",
      "Iteration 151, loss = 0.02174141\n",
      "Iteration 152, loss = 0.02155519\n",
      "Iteration 153, loss = 0.02130281\n",
      "Iteration 154, loss = 0.02100808\n",
      "Iteration 155, loss = 0.02077894\n",
      "Iteration 156, loss = 0.02060189\n",
      "Iteration 157, loss = 0.02034126\n",
      "Iteration 158, loss = 0.02014725\n",
      "Iteration 159, loss = 0.01987655\n",
      "Iteration 160, loss = 0.01969923\n",
      "Iteration 161, loss = 0.01947279\n",
      "Iteration 162, loss = 0.01922094\n",
      "Iteration 163, loss = 0.01905784\n",
      "Iteration 164, loss = 0.01885360\n",
      "Iteration 165, loss = 0.01861702\n",
      "Iteration 166, loss = 0.01852304\n",
      "Iteration 167, loss = 0.01822289\n",
      "Iteration 168, loss = 0.01802077\n",
      "Iteration 169, loss = 0.01790213\n",
      "Iteration 170, loss = 0.01763805\n",
      "Iteration 171, loss = 0.01749917\n",
      "Iteration 172, loss = 0.01724664\n",
      "Iteration 173, loss = 0.01710458\n",
      "Iteration 174, loss = 0.01697456\n",
      "Iteration 175, loss = 0.01684325\n",
      "Iteration 176, loss = 0.01660952\n",
      "Iteration 177, loss = 0.01637915\n",
      "Iteration 178, loss = 0.01618221\n",
      "Iteration 179, loss = 0.01604690\n",
      "Iteration 180, loss = 0.01583726\n",
      "Iteration 181, loss = 0.01573748\n",
      "Iteration 182, loss = 0.01553937\n",
      "Iteration 183, loss = 0.01544698\n",
      "Iteration 184, loss = 0.01526609\n",
      "Iteration 185, loss = 0.01503251\n",
      "Iteration 186, loss = 0.01492002\n",
      "Iteration 187, loss = 0.01469664\n",
      "Iteration 188, loss = 0.01454028\n",
      "Iteration 189, loss = 0.01439263\n",
      "Iteration 190, loss = 0.01424330\n",
      "Iteration 191, loss = 0.01419017\n",
      "Iteration 192, loss = 0.01397176\n",
      "Iteration 193, loss = 0.01382492\n",
      "Iteration 194, loss = 0.01367469\n",
      "Iteration 195, loss = 0.01354772\n",
      "Iteration 196, loss = 0.01339648\n",
      "Iteration 197, loss = 0.01333423\n",
      "Iteration 198, loss = 0.01318245\n",
      "Iteration 199, loss = 0.01299349\n",
      "Iteration 200, loss = 0.01298133\n",
      "Iteration 201, loss = 0.01281823\n",
      "Iteration 202, loss = 0.01269572\n",
      "Iteration 203, loss = 0.01250181\n",
      "Iteration 204, loss = 0.01237771\n",
      "Iteration 205, loss = 0.01236976\n",
      "Iteration 206, loss = 0.01219211\n",
      "Iteration 207, loss = 0.01201842\n",
      "Iteration 208, loss = 0.01199044\n",
      "Iteration 209, loss = 0.01185205\n",
      "Iteration 210, loss = 0.01170122\n",
      "Iteration 211, loss = 0.01162877\n",
      "Iteration 212, loss = 0.01152907\n",
      "Iteration 213, loss = 0.01140952\n",
      "Iteration 214, loss = 0.01123642\n",
      "Iteration 215, loss = 0.01119035\n",
      "Iteration 216, loss = 0.01105951\n",
      "Iteration 217, loss = 0.01099019\n",
      "Iteration 218, loss = 0.01082498\n",
      "Iteration 219, loss = 0.01074425\n",
      "Iteration 220, loss = 0.01070555\n",
      "Iteration 221, loss = 0.01059351\n",
      "Iteration 222, loss = 0.01048087\n",
      "Iteration 223, loss = 0.01034068\n",
      "Iteration 224, loss = 0.01033965\n",
      "Iteration 225, loss = 0.01020558\n",
      "Iteration 226, loss = 0.01013238\n",
      "Iteration 227, loss = 0.01005133\n",
      "Iteration 228, loss = 0.00991846\n",
      "Iteration 229, loss = 0.00981857\n",
      "Iteration 230, loss = 0.00977919\n",
      "Iteration 231, loss = 0.00965741\n",
      "Iteration 232, loss = 0.00957118\n",
      "Iteration 233, loss = 0.00951373\n",
      "Iteration 234, loss = 0.00942097\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98796543\n",
      "Iteration 2, loss = 1.04285849\n",
      "Iteration 3, loss = 0.74055902\n",
      "Iteration 4, loss = 0.59323528\n",
      "Iteration 5, loss = 0.50077323\n",
      "Iteration 6, loss = 0.44049095\n",
      "Iteration 7, loss = 0.39371918\n",
      "Iteration 8, loss = 0.35932155\n",
      "Iteration 9, loss = 0.33086019\n",
      "Iteration 10, loss = 0.30883259\n",
      "Iteration 11, loss = 0.28737439\n",
      "Iteration 12, loss = 0.27029636\n",
      "Iteration 13, loss = 0.25621441\n",
      "Iteration 14, loss = 0.24348942\n",
      "Iteration 15, loss = 0.23084306\n",
      "Iteration 16, loss = 0.22161164\n",
      "Iteration 17, loss = 0.21118817\n",
      "Iteration 18, loss = 0.20310198\n",
      "Iteration 19, loss = 0.19502469\n",
      "Iteration 20, loss = 0.18785237\n",
      "Iteration 21, loss = 0.18065935\n",
      "Iteration 22, loss = 0.17525116\n",
      "Iteration 23, loss = 0.17028912\n",
      "Iteration 24, loss = 0.16513484\n",
      "Iteration 25, loss = 0.16063464\n",
      "Iteration 26, loss = 0.15599655\n",
      "Iteration 27, loss = 0.15116752\n",
      "Iteration 28, loss = 0.14772084\n",
      "Iteration 29, loss = 0.14430881\n",
      "Iteration 30, loss = 0.14094337\n",
      "Iteration 31, loss = 0.13733396\n",
      "Iteration 32, loss = 0.13493919\n",
      "Iteration 33, loss = 0.13136256\n",
      "Iteration 34, loss = 0.12840710\n",
      "Iteration 35, loss = 0.12580675\n",
      "Iteration 36, loss = 0.12349751\n",
      "Iteration 37, loss = 0.12101330\n",
      "Iteration 38, loss = 0.11879362\n",
      "Iteration 39, loss = 0.11671199\n",
      "Iteration 40, loss = 0.11406391\n",
      "Iteration 41, loss = 0.11297071\n",
      "Iteration 42, loss = 0.11044358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.10851841\n",
      "Iteration 44, loss = 0.10702622\n",
      "Iteration 45, loss = 0.10488961\n",
      "Iteration 46, loss = 0.10312905\n",
      "Iteration 47, loss = 0.10148857\n",
      "Iteration 48, loss = 0.09992305\n",
      "Iteration 49, loss = 0.09872262\n",
      "Iteration 50, loss = 0.09659534\n",
      "Iteration 51, loss = 0.09529181\n",
      "Iteration 52, loss = 0.09384764\n",
      "Iteration 53, loss = 0.09238268\n",
      "Iteration 54, loss = 0.09122932\n",
      "Iteration 55, loss = 0.08986690\n",
      "Iteration 56, loss = 0.08870875\n",
      "Iteration 57, loss = 0.08774492\n",
      "Iteration 58, loss = 0.08661297\n",
      "Iteration 59, loss = 0.08515060\n",
      "Iteration 60, loss = 0.08368635\n",
      "Iteration 61, loss = 0.08259912\n",
      "Iteration 62, loss = 0.08179667\n",
      "Iteration 63, loss = 0.08049553\n",
      "Iteration 64, loss = 0.07956951\n",
      "Iteration 65, loss = 0.07878609\n",
      "Iteration 66, loss = 0.07782891\n",
      "Iteration 67, loss = 0.07658835\n",
      "Iteration 68, loss = 0.07566631\n",
      "Iteration 69, loss = 0.07476103\n",
      "Iteration 70, loss = 0.07395310\n",
      "Iteration 71, loss = 0.07279248\n",
      "Iteration 72, loss = 0.07186698\n",
      "Iteration 73, loss = 0.07115945\n",
      "Iteration 74, loss = 0.07031985\n",
      "Iteration 75, loss = 0.06950771\n",
      "Iteration 76, loss = 0.06870967\n",
      "Iteration 77, loss = 0.06749690\n",
      "Iteration 78, loss = 0.06714511\n",
      "Iteration 79, loss = 0.06633739\n",
      "Iteration 80, loss = 0.06551178\n",
      "Iteration 81, loss = 0.06438034\n",
      "Iteration 82, loss = 0.06378002\n",
      "Iteration 83, loss = 0.06331977\n",
      "Iteration 84, loss = 0.06273929\n",
      "Iteration 85, loss = 0.06182216\n",
      "Iteration 86, loss = 0.06118920\n",
      "Iteration 87, loss = 0.06059985\n",
      "Iteration 88, loss = 0.05996986\n",
      "Iteration 89, loss = 0.05936284\n",
      "Iteration 90, loss = 0.05837824\n",
      "Iteration 91, loss = 0.05828182\n",
      "Iteration 92, loss = 0.05730333\n",
      "Iteration 93, loss = 0.05721778\n",
      "Iteration 94, loss = 0.05605414\n",
      "Iteration 95, loss = 0.05549122\n",
      "Iteration 96, loss = 0.05536948\n",
      "Iteration 97, loss = 0.05421875\n",
      "Iteration 98, loss = 0.05374168\n",
      "Iteration 99, loss = 0.05308764\n",
      "Iteration 100, loss = 0.05276051\n",
      "Iteration 101, loss = 0.05182792\n",
      "Iteration 102, loss = 0.05167082\n",
      "Iteration 103, loss = 0.05087555\n",
      "Iteration 104, loss = 0.05063714\n",
      "Iteration 105, loss = 0.05031097\n",
      "Iteration 106, loss = 0.04949382\n",
      "Iteration 107, loss = 0.04910276\n",
      "Iteration 108, loss = 0.04869254\n",
      "Iteration 109, loss = 0.04799095\n",
      "Iteration 110, loss = 0.04752068\n",
      "Iteration 111, loss = 0.04699551\n",
      "Iteration 112, loss = 0.04643217\n",
      "Iteration 113, loss = 0.04623546\n",
      "Iteration 114, loss = 0.04573341\n",
      "Iteration 115, loss = 0.04532713\n",
      "Iteration 116, loss = 0.04473530\n",
      "Iteration 117, loss = 0.04427919\n",
      "Iteration 118, loss = 0.04385122\n",
      "Iteration 119, loss = 0.04352484\n",
      "Iteration 120, loss = 0.04298016\n",
      "Iteration 121, loss = 0.04266576\n",
      "Iteration 122, loss = 0.04245898\n",
      "Iteration 123, loss = 0.04171287\n",
      "Iteration 124, loss = 0.04136944\n",
      "Iteration 125, loss = 0.04104667\n",
      "Iteration 126, loss = 0.04030417\n",
      "Iteration 127, loss = 0.04031746\n",
      "Iteration 128, loss = 0.03965264\n",
      "Iteration 129, loss = 0.03926825\n",
      "Iteration 130, loss = 0.03884396\n",
      "Iteration 131, loss = 0.03897261\n",
      "Iteration 132, loss = 0.03823404\n",
      "Iteration 133, loss = 0.03774910\n",
      "Iteration 134, loss = 0.03746590\n",
      "Iteration 135, loss = 0.03694395\n",
      "Iteration 136, loss = 0.03682166\n",
      "Iteration 137, loss = 0.03626877\n",
      "Iteration 138, loss = 0.03605033\n",
      "Iteration 139, loss = 0.03563135\n",
      "Iteration 140, loss = 0.03516406\n",
      "Iteration 141, loss = 0.03498612\n",
      "Iteration 142, loss = 0.03486591\n",
      "Iteration 143, loss = 0.03440773\n",
      "Iteration 144, loss = 0.03425685\n",
      "Iteration 145, loss = 0.03367065\n",
      "Iteration 146, loss = 0.03335715\n",
      "Iteration 147, loss = 0.03310806\n",
      "Iteration 148, loss = 0.03260882\n",
      "Iteration 149, loss = 0.03254624\n",
      "Iteration 150, loss = 0.03239177\n",
      "Iteration 151, loss = 0.03198297\n",
      "Iteration 152, loss = 0.03170509\n",
      "Iteration 153, loss = 0.03143361\n",
      "Iteration 154, loss = 0.03118359\n",
      "Iteration 155, loss = 0.03103442\n",
      "Iteration 156, loss = 0.03041978\n",
      "Iteration 157, loss = 0.03046350\n",
      "Iteration 158, loss = 0.03000724\n",
      "Iteration 159, loss = 0.02980645\n",
      "Iteration 160, loss = 0.02950369\n",
      "Iteration 161, loss = 0.02916044\n",
      "Iteration 162, loss = 0.02888642\n",
      "Iteration 163, loss = 0.02875877\n",
      "Iteration 164, loss = 0.02849433\n",
      "Iteration 165, loss = 0.02819404\n",
      "Iteration 166, loss = 0.02782030\n",
      "Iteration 167, loss = 0.02782220\n",
      "Iteration 168, loss = 0.02732437\n",
      "Iteration 169, loss = 0.02721579\n",
      "Iteration 170, loss = 0.02690201\n",
      "Iteration 171, loss = 0.02671859\n",
      "Iteration 172, loss = 0.02648000\n",
      "Iteration 173, loss = 0.02620191\n",
      "Iteration 174, loss = 0.02601280\n",
      "Iteration 175, loss = 0.02573029\n",
      "Iteration 176, loss = 0.02564671\n",
      "Iteration 177, loss = 0.02529316\n",
      "Iteration 178, loss = 0.02523154\n",
      "Iteration 179, loss = 0.02517748\n",
      "Iteration 180, loss = 0.02467150\n",
      "Iteration 181, loss = 0.02450730\n",
      "Iteration 182, loss = 0.02428803\n",
      "Iteration 183, loss = 0.02403768\n",
      "Iteration 184, loss = 0.02394863\n",
      "Iteration 185, loss = 0.02359140\n",
      "Iteration 186, loss = 0.02359453\n",
      "Iteration 187, loss = 0.02325754\n",
      "Iteration 188, loss = 0.02329354\n",
      "Iteration 189, loss = 0.02284281\n",
      "Iteration 190, loss = 0.02295608\n",
      "Iteration 191, loss = 0.02252561\n",
      "Iteration 192, loss = 0.02237193\n",
      "Iteration 193, loss = 0.02224812\n",
      "Iteration 194, loss = 0.02195838\n",
      "Iteration 195, loss = 0.02174938\n",
      "Iteration 196, loss = 0.02181307\n",
      "Iteration 197, loss = 0.02155167\n",
      "Iteration 198, loss = 0.02141445\n",
      "Iteration 199, loss = 0.02122887\n",
      "Iteration 200, loss = 0.02096852\n",
      "Iteration 201, loss = 0.02079041\n",
      "Iteration 202, loss = 0.02076284\n",
      "Iteration 203, loss = 0.02061917\n",
      "Iteration 204, loss = 0.02029590\n",
      "Iteration 205, loss = 0.02028576\n",
      "Iteration 206, loss = 0.01997671\n",
      "Iteration 207, loss = 0.01984448\n",
      "Iteration 208, loss = 0.01955095\n",
      "Iteration 209, loss = 0.01956454\n",
      "Iteration 210, loss = 0.01943537\n",
      "Iteration 211, loss = 0.01922694\n",
      "Iteration 212, loss = 0.01900081\n",
      "Iteration 213, loss = 0.01891775\n",
      "Iteration 214, loss = 0.01872187\n",
      "Iteration 215, loss = 0.01856082\n",
      "Iteration 216, loss = 0.01842806\n",
      "Iteration 217, loss = 0.01833660\n",
      "Iteration 218, loss = 0.01817956\n",
      "Iteration 219, loss = 0.01802211\n",
      "Iteration 220, loss = 0.01784675\n",
      "Iteration 221, loss = 0.01777517\n",
      "Iteration 222, loss = 0.01756856\n",
      "Iteration 223, loss = 0.01738695\n",
      "Iteration 224, loss = 0.01745304\n",
      "Iteration 225, loss = 0.01706854\n",
      "Iteration 226, loss = 0.01702641\n",
      "Iteration 227, loss = 0.01688345\n",
      "Iteration 228, loss = 0.01676972\n",
      "Iteration 229, loss = 0.01657822\n",
      "Iteration 230, loss = 0.01640288\n",
      "Iteration 231, loss = 0.01638005\n",
      "Iteration 232, loss = 0.01617232\n",
      "Iteration 233, loss = 0.01612755\n",
      "Iteration 234, loss = 0.01602427\n",
      "Iteration 235, loss = 0.01577624\n",
      "Iteration 236, loss = 0.01567453\n",
      "Iteration 237, loss = 0.01562855\n",
      "Iteration 238, loss = 0.01550736\n",
      "Iteration 239, loss = 0.01550373\n",
      "Iteration 240, loss = 0.01520157\n",
      "Iteration 241, loss = 0.01519053\n",
      "Iteration 242, loss = 0.01502237\n",
      "Iteration 243, loss = 0.01495361\n",
      "Iteration 244, loss = 0.01480830\n",
      "Iteration 245, loss = 0.01469955\n",
      "Iteration 246, loss = 0.01462456\n",
      "Iteration 247, loss = 0.01446694\n",
      "Iteration 248, loss = 0.01442861\n",
      "Iteration 249, loss = 0.01431776\n",
      "Iteration 250, loss = 0.01413807\n",
      "Iteration 251, loss = 0.01403754\n",
      "Iteration 252, loss = 0.01389853\n",
      "Iteration 253, loss = 0.01374647\n",
      "Iteration 254, loss = 0.01369631\n",
      "Iteration 255, loss = 0.01354260\n",
      "Iteration 256, loss = 0.01351215\n",
      "Iteration 257, loss = 0.01339097\n",
      "Iteration 258, loss = 0.01330873\n",
      "Iteration 259, loss = 0.01320831\n",
      "Iteration 260, loss = 0.01314278\n",
      "Iteration 261, loss = 0.01304795\n",
      "Iteration 262, loss = 0.01287273\n",
      "Iteration 263, loss = 0.01286184\n",
      "Iteration 264, loss = 0.01265327\n",
      "Iteration 265, loss = 0.01264700\n",
      "Iteration 266, loss = 0.01250529\n",
      "Iteration 267, loss = 0.01241498\n",
      "Iteration 268, loss = 0.01228740\n",
      "Iteration 269, loss = 0.01226544\n",
      "Iteration 270, loss = 0.01221572\n",
      "Iteration 271, loss = 0.01203738\n",
      "Iteration 272, loss = 0.01193208\n",
      "Iteration 273, loss = 0.01197530\n",
      "Iteration 274, loss = 0.01176053\n",
      "Iteration 275, loss = 0.01168908\n",
      "Iteration 276, loss = 0.01162562\n",
      "Iteration 277, loss = 0.01153506\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03971678\n",
      "Iteration 2, loss = 1.05349107\n",
      "Iteration 3, loss = 0.74907726\n",
      "Iteration 4, loss = 0.59373710\n",
      "Iteration 5, loss = 0.50113675\n",
      "Iteration 6, loss = 0.43971919\n",
      "Iteration 7, loss = 0.39369899\n",
      "Iteration 8, loss = 0.35780696\n",
      "Iteration 9, loss = 0.32893138\n",
      "Iteration 10, loss = 0.30673203\n",
      "Iteration 11, loss = 0.28548527\n",
      "Iteration 12, loss = 0.26962041\n",
      "Iteration 13, loss = 0.25433817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.24074164\n",
      "Iteration 15, loss = 0.22958794\n",
      "Iteration 16, loss = 0.21917135\n",
      "Iteration 17, loss = 0.20987096\n",
      "Iteration 18, loss = 0.28216631\n",
      "Iteration 19, loss = 0.20655991\n",
      "Iteration 20, loss = 0.19299118\n",
      "Iteration 21, loss = 0.18369977\n",
      "Iteration 22, loss = 0.17615817\n",
      "Iteration 23, loss = 0.16966567\n",
      "Iteration 24, loss = 0.16719259\n",
      "Iteration 25, loss = 0.16752135\n",
      "Iteration 26, loss = 0.15515464\n",
      "Iteration 27, loss = 0.15069076\n",
      "Iteration 28, loss = 0.14603168\n",
      "Iteration 29, loss = 0.14258836\n",
      "Iteration 30, loss = 0.13853756\n",
      "Iteration 31, loss = 0.13519240\n",
      "Iteration 32, loss = 0.13201828\n",
      "Iteration 33, loss = 0.12863299\n",
      "Iteration 34, loss = 0.12585709\n",
      "Iteration 35, loss = 0.12318761\n",
      "Iteration 36, loss = 0.12038689\n",
      "Iteration 37, loss = 0.11788955\n",
      "Iteration 38, loss = 0.11519656\n",
      "Iteration 39, loss = 0.11293015\n",
      "Iteration 40, loss = 0.11044900\n",
      "Iteration 41, loss = 0.10846813\n",
      "Iteration 42, loss = 0.10668202\n",
      "Iteration 43, loss = 0.10424486\n",
      "Iteration 44, loss = 0.10251915\n",
      "Iteration 45, loss = 0.10075007\n",
      "Iteration 46, loss = 0.09864408\n",
      "Iteration 47, loss = 0.09716889\n",
      "Iteration 48, loss = 0.09515359\n",
      "Iteration 49, loss = 0.09400005\n",
      "Iteration 50, loss = 0.09260910\n",
      "Iteration 51, loss = 0.09047357\n",
      "Iteration 52, loss = 0.08937052\n",
      "Iteration 53, loss = 0.08764532\n",
      "Iteration 54, loss = 0.08638287\n",
      "Iteration 55, loss = 0.08477751\n",
      "Iteration 56, loss = 0.08327242\n",
      "Iteration 57, loss = 0.08301921\n",
      "Iteration 58, loss = 0.08110554\n",
      "Iteration 59, loss = 0.07958933\n",
      "Iteration 60, loss = 0.07891812\n",
      "Iteration 61, loss = 0.07729608\n",
      "Iteration 62, loss = 0.07631960\n",
      "Iteration 63, loss = 0.08951208\n",
      "Iteration 64, loss = 0.07569010\n",
      "Iteration 65, loss = 0.07374397\n",
      "Iteration 66, loss = 0.07220789\n",
      "Iteration 67, loss = 0.07113910\n",
      "Iteration 68, loss = 0.07013851\n",
      "Iteration 69, loss = 0.06909637\n",
      "Iteration 70, loss = 0.07147659\n",
      "Iteration 71, loss = 0.06763770\n",
      "Iteration 72, loss = 0.06635355\n",
      "Iteration 73, loss = 0.06541303\n",
      "Iteration 74, loss = 0.06459230\n",
      "Iteration 75, loss = 0.06367716\n",
      "Iteration 76, loss = 0.06298613\n",
      "Iteration 77, loss = 0.06198764\n",
      "Iteration 78, loss = 0.06141592\n",
      "Iteration 79, loss = 0.06230091\n",
      "Iteration 80, loss = 0.05954593\n",
      "Iteration 81, loss = 0.05908962\n",
      "Iteration 82, loss = 0.05800590\n",
      "Iteration 83, loss = 0.05732617\n",
      "Iteration 84, loss = 0.05671551\n",
      "Iteration 85, loss = 0.05574948\n",
      "Iteration 86, loss = 0.05552784\n",
      "Iteration 87, loss = 0.05459589\n",
      "Iteration 88, loss = 0.05387712\n",
      "Iteration 89, loss = 0.05331074\n",
      "Iteration 90, loss = 0.05259816\n",
      "Iteration 91, loss = 0.05226831\n",
      "Iteration 92, loss = 0.05148007\n",
      "Iteration 93, loss = 0.05121007\n",
      "Iteration 94, loss = 0.05027500\n",
      "Iteration 95, loss = 0.04955199\n",
      "Iteration 96, loss = 0.04925748\n",
      "Iteration 97, loss = 0.04885969\n",
      "Iteration 98, loss = 0.04782027\n",
      "Iteration 99, loss = 0.04744904\n",
      "Iteration 100, loss = 0.04688067\n",
      "Iteration 101, loss = 0.04632451\n",
      "Iteration 102, loss = 0.04560284\n",
      "Iteration 103, loss = 0.04775734\n",
      "Iteration 104, loss = 0.04502780\n",
      "Iteration 105, loss = 0.04470748\n",
      "Iteration 106, loss = 0.04379246\n",
      "Iteration 107, loss = 0.04350898\n",
      "Iteration 108, loss = 0.04295825\n",
      "Iteration 109, loss = 0.04269967\n",
      "Iteration 110, loss = 0.04204118\n",
      "Iteration 111, loss = 0.04158977\n",
      "Iteration 112, loss = 0.04433139\n",
      "Iteration 113, loss = 0.04110261\n",
      "Iteration 114, loss = 0.04040679\n",
      "Iteration 115, loss = 0.03995028\n",
      "Iteration 116, loss = 0.03959347\n",
      "Iteration 117, loss = 0.03892772\n",
      "Iteration 118, loss = 0.03880942\n",
      "Iteration 119, loss = 0.03823669\n",
      "Iteration 120, loss = 0.03778201\n",
      "Iteration 121, loss = 0.03748148\n",
      "Iteration 122, loss = 0.03693277\n",
      "Iteration 123, loss = 0.03671410\n",
      "Iteration 124, loss = 0.03654761\n",
      "Iteration 125, loss = 0.03590013\n",
      "Iteration 126, loss = 0.03559815\n",
      "Iteration 127, loss = 0.03507528\n",
      "Iteration 128, loss = 0.03486840\n",
      "Iteration 129, loss = 0.03474519\n",
      "Iteration 130, loss = 0.03411814\n",
      "Iteration 131, loss = 0.03372168\n",
      "Iteration 132, loss = 0.03338949\n",
      "Iteration 133, loss = 0.03300337\n",
      "Iteration 134, loss = 0.03291602\n",
      "Iteration 135, loss = 0.03251976\n",
      "Iteration 136, loss = 0.03223682\n",
      "Iteration 137, loss = 0.03179868\n",
      "Iteration 138, loss = 0.03154463\n",
      "Iteration 139, loss = 0.03132807\n",
      "Iteration 140, loss = 0.03090832\n",
      "Iteration 141, loss = 0.03069977\n",
      "Iteration 142, loss = 0.03184430\n",
      "Iteration 143, loss = 0.03018449\n",
      "Iteration 144, loss = 0.02983554\n",
      "Iteration 145, loss = 0.02956102\n",
      "Iteration 146, loss = 0.02924391\n",
      "Iteration 147, loss = 0.02909768\n",
      "Iteration 148, loss = 0.02874211\n",
      "Iteration 149, loss = 0.02849711\n",
      "Iteration 150, loss = 0.02815879\n",
      "Iteration 151, loss = 0.02782750\n",
      "Iteration 152, loss = 0.02762738\n",
      "Iteration 153, loss = 0.02734132\n",
      "Iteration 154, loss = 0.02714428\n",
      "Iteration 155, loss = 0.02685023\n",
      "Iteration 156, loss = 0.02679343\n",
      "Iteration 157, loss = 0.02659901\n",
      "Iteration 158, loss = 0.02614983\n",
      "Iteration 159, loss = 0.02583281\n",
      "Iteration 160, loss = 0.02578003\n",
      "Iteration 161, loss = 0.02535878\n",
      "Iteration 162, loss = 0.02518728\n",
      "Iteration 163, loss = 0.02500771\n",
      "Iteration 164, loss = 0.02656224\n",
      "Iteration 165, loss = 0.02485212\n",
      "Iteration 166, loss = 0.02462218\n",
      "Iteration 167, loss = 0.02409172\n",
      "Iteration 168, loss = 0.02378590\n",
      "Iteration 169, loss = 0.02375873\n",
      "Iteration 170, loss = 0.02365347\n",
      "Iteration 171, loss = 0.02329843\n",
      "Iteration 172, loss = 0.02309667\n",
      "Iteration 173, loss = 0.02293613\n",
      "Iteration 174, loss = 0.02284302\n",
      "Iteration 175, loss = 0.02262024\n",
      "Iteration 176, loss = 0.02249864\n",
      "Iteration 177, loss = 0.02211597\n",
      "Iteration 178, loss = 0.02202813\n",
      "Iteration 179, loss = 0.02191305\n",
      "Iteration 180, loss = 0.02265961\n",
      "Iteration 181, loss = 0.02154014\n",
      "Iteration 182, loss = 0.02133914\n",
      "Iteration 183, loss = 0.02812579\n",
      "Iteration 184, loss = 0.02152548\n",
      "Iteration 185, loss = 0.02109786\n",
      "Iteration 186, loss = 0.02073932\n",
      "Iteration 187, loss = 0.02154323\n",
      "Iteration 188, loss = 0.02051016\n",
      "Iteration 189, loss = 0.02018847\n",
      "Iteration 190, loss = 0.01997282\n",
      "Iteration 191, loss = 0.01978405\n",
      "Iteration 192, loss = 0.01960302\n",
      "Iteration 193, loss = 0.01939760\n",
      "Iteration 194, loss = 0.01928154\n",
      "Iteration 195, loss = 0.01908378\n",
      "Iteration 196, loss = 0.01894976\n",
      "Iteration 197, loss = 0.01871344\n",
      "Iteration 198, loss = 0.01865664\n",
      "Iteration 199, loss = 0.01834365\n",
      "Iteration 200, loss = 0.01841567\n",
      "Iteration 201, loss = 0.01812215\n",
      "Iteration 202, loss = 0.01798901\n",
      "Iteration 203, loss = 0.01780874\n",
      "Iteration 204, loss = 0.01782652\n",
      "Iteration 205, loss = 0.01752502\n",
      "Iteration 206, loss = 0.01741082\n",
      "Iteration 207, loss = 0.01716171\n",
      "Iteration 208, loss = 0.01717481\n",
      "Iteration 209, loss = 0.01695834\n",
      "Iteration 210, loss = 0.01687353\n",
      "Iteration 211, loss = 0.01668123\n",
      "Iteration 212, loss = 0.01791473\n",
      "Iteration 213, loss = 0.01665886\n",
      "Iteration 214, loss = 0.01647349\n",
      "Iteration 215, loss = 0.01636897\n",
      "Iteration 216, loss = 0.01606813\n",
      "Iteration 217, loss = 0.01601177\n",
      "Iteration 218, loss = 0.01590071\n",
      "Iteration 219, loss = 0.01575442\n",
      "Iteration 220, loss = 0.01568228\n",
      "Iteration 221, loss = 0.01557808\n",
      "Iteration 222, loss = 0.01542233\n",
      "Iteration 223, loss = 0.01523658\n",
      "Iteration 224, loss = 0.01516720\n",
      "Iteration 225, loss = 0.01510520\n",
      "Iteration 226, loss = 0.01495540\n",
      "Iteration 227, loss = 0.01484866\n",
      "Iteration 228, loss = 0.01478218\n",
      "Iteration 229, loss = 0.01458368\n",
      "Iteration 230, loss = 0.01453112\n",
      "Iteration 231, loss = 0.01437505\n",
      "Iteration 232, loss = 0.01434206\n",
      "Iteration 233, loss = 0.01416478\n",
      "Iteration 234, loss = 0.01401710\n",
      "Iteration 235, loss = 0.01399300\n",
      "Iteration 236, loss = 0.01380227\n",
      "Iteration 237, loss = 0.01378580\n",
      "Iteration 238, loss = 0.01357443\n",
      "Iteration 239, loss = 0.01350520\n",
      "Iteration 240, loss = 0.01344672\n",
      "Iteration 241, loss = 0.01337477\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98778090\n",
      "Iteration 2, loss = 1.04170760\n",
      "Iteration 3, loss = 0.74579089\n",
      "Iteration 4, loss = 0.59269631\n",
      "Iteration 5, loss = 0.51767037\n",
      "Iteration 6, loss = 0.47032246\n",
      "Iteration 7, loss = 0.39308459\n",
      "Iteration 8, loss = 0.35572864\n",
      "Iteration 9, loss = 0.32512011\n",
      "Iteration 10, loss = 0.30007476\n",
      "Iteration 11, loss = 0.28040577\n",
      "Iteration 12, loss = 0.26142987\n",
      "Iteration 13, loss = 0.24648197\n",
      "Iteration 14, loss = 0.23412255\n",
      "Iteration 15, loss = 0.22480747\n",
      "Iteration 16, loss = 0.21106667\n",
      "Iteration 17, loss = 0.20195144\n",
      "Iteration 18, loss = 0.19520830\n",
      "Iteration 19, loss = 0.18562779\n",
      "Iteration 20, loss = 0.17916918\n",
      "Iteration 21, loss = 0.17481149\n",
      "Iteration 22, loss = 0.16623350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.16403257\n",
      "Iteration 24, loss = 0.15466448\n",
      "Iteration 25, loss = 0.15010026\n",
      "Iteration 26, loss = 0.14519534\n",
      "Iteration 27, loss = 0.14930232\n",
      "Iteration 28, loss = 0.13809286\n",
      "Iteration 29, loss = 0.13355221\n",
      "Iteration 30, loss = 0.12976738\n",
      "Iteration 31, loss = 0.14380323\n",
      "Iteration 32, loss = 0.12472342\n",
      "Iteration 33, loss = 0.12455208\n",
      "Iteration 34, loss = 0.11816142\n",
      "Iteration 35, loss = 0.11532507\n",
      "Iteration 36, loss = 0.11240374\n",
      "Iteration 37, loss = 0.11003402\n",
      "Iteration 38, loss = 0.10821929\n",
      "Iteration 39, loss = 0.11556795\n",
      "Iteration 40, loss = 0.10364254\n",
      "Iteration 41, loss = 0.10125543\n",
      "Iteration 42, loss = 0.09896737\n",
      "Iteration 43, loss = 0.09696274\n",
      "Iteration 44, loss = 0.09480080\n",
      "Iteration 45, loss = 0.09303915\n",
      "Iteration 46, loss = 0.09134598\n",
      "Iteration 47, loss = 0.08963861\n",
      "Iteration 48, loss = 0.08784485\n",
      "Iteration 49, loss = 0.08624453\n",
      "Iteration 50, loss = 0.09215761\n",
      "Iteration 51, loss = 0.08418022\n",
      "Iteration 52, loss = 0.08221780\n",
      "Iteration 53, loss = 0.08075865\n",
      "Iteration 54, loss = 0.07915890\n",
      "Iteration 55, loss = 0.07786929\n",
      "Iteration 56, loss = 0.07687138\n",
      "Iteration 57, loss = 0.07515042\n",
      "Iteration 58, loss = 0.07444024\n",
      "Iteration 59, loss = 0.07277359\n",
      "Iteration 60, loss = 0.07186495\n",
      "Iteration 61, loss = 0.07064898\n",
      "Iteration 62, loss = 0.07485781\n",
      "Iteration 63, loss = 0.06872848\n",
      "Iteration 64, loss = 0.06750521\n",
      "Iteration 65, loss = 0.06664222\n",
      "Iteration 66, loss = 0.06564367\n",
      "Iteration 67, loss = 0.06438434\n",
      "Iteration 68, loss = 0.06391228\n",
      "Iteration 69, loss = 0.06231728\n",
      "Iteration 70, loss = 0.06144418\n",
      "Iteration 71, loss = 0.06041054\n",
      "Iteration 72, loss = 0.05960134\n",
      "Iteration 73, loss = 0.05863486\n",
      "Iteration 74, loss = 0.05797064\n",
      "Iteration 75, loss = 0.05812262\n",
      "Iteration 76, loss = 0.05653666\n",
      "Iteration 77, loss = 0.05554040\n",
      "Iteration 78, loss = 0.05475263\n",
      "Iteration 79, loss = 0.05389166\n",
      "Iteration 80, loss = 0.05327240\n",
      "Iteration 81, loss = 0.05248690\n",
      "Iteration 82, loss = 0.05174662\n",
      "Iteration 83, loss = 0.05095011\n",
      "Iteration 84, loss = 0.05295931\n",
      "Iteration 85, loss = 0.04985906\n",
      "Iteration 86, loss = 0.04887041\n",
      "Iteration 87, loss = 0.04855774\n",
      "Iteration 88, loss = 0.04795940\n",
      "Iteration 89, loss = 0.04736683\n",
      "Iteration 90, loss = 0.04629678\n",
      "Iteration 91, loss = 0.04585370\n",
      "Iteration 92, loss = 0.04519787\n",
      "Iteration 93, loss = 0.04491709\n",
      "Iteration 94, loss = 0.06572841\n",
      "Iteration 95, loss = 0.04592166\n",
      "Iteration 96, loss = 0.04441016\n",
      "Iteration 97, loss = 0.04637839\n",
      "Iteration 98, loss = 0.04266561\n",
      "Iteration 99, loss = 0.04156897\n",
      "Iteration 100, loss = 0.04149189\n",
      "Iteration 101, loss = 0.04028916\n",
      "Iteration 102, loss = 0.03986740\n",
      "Iteration 103, loss = 0.03953279\n",
      "Iteration 104, loss = 0.03864750\n",
      "Iteration 105, loss = 0.03824771\n",
      "Iteration 106, loss = 0.03771758\n",
      "Iteration 107, loss = 0.03750193\n",
      "Iteration 108, loss = 0.03674870\n",
      "Iteration 109, loss = 0.03616130\n",
      "Iteration 110, loss = 0.03598196\n",
      "Iteration 111, loss = 0.03587613\n",
      "Iteration 112, loss = 0.03531970\n",
      "Iteration 113, loss = 0.03483310\n",
      "Iteration 114, loss = 0.03404074\n",
      "Iteration 115, loss = 0.03601485\n",
      "Iteration 116, loss = 0.03365620\n",
      "Iteration 117, loss = 0.03307430\n",
      "Iteration 118, loss = 0.03283518\n",
      "Iteration 119, loss = 0.03230002\n",
      "Iteration 120, loss = 0.03193374\n",
      "Iteration 121, loss = 0.03154994\n",
      "Iteration 122, loss = 0.03109489\n",
      "Iteration 123, loss = 0.03078572\n",
      "Iteration 124, loss = 0.03038593\n",
      "Iteration 125, loss = 0.03014986\n",
      "Iteration 126, loss = 0.02976201\n",
      "Iteration 127, loss = 0.02927045\n",
      "Iteration 128, loss = 0.03122470\n",
      "Iteration 129, loss = 0.02899454\n",
      "Iteration 130, loss = 0.02838622\n",
      "Iteration 131, loss = 0.02815627\n",
      "Iteration 132, loss = 0.02773666\n",
      "Iteration 133, loss = 0.02798970\n",
      "Iteration 134, loss = 0.02721623\n",
      "Iteration 135, loss = 0.02693374\n",
      "Iteration 136, loss = 0.02654090\n",
      "Iteration 137, loss = 0.02630321\n",
      "Iteration 138, loss = 0.02606177\n",
      "Iteration 139, loss = 0.02568708\n",
      "Iteration 140, loss = 0.02532750\n",
      "Iteration 141, loss = 0.02515585\n",
      "Iteration 142, loss = 0.02485875\n",
      "Iteration 143, loss = 0.02453322\n",
      "Iteration 144, loss = 0.02426740\n",
      "Iteration 145, loss = 0.02406475\n",
      "Iteration 146, loss = 0.02390395\n",
      "Iteration 147, loss = 0.02368531\n",
      "Iteration 148, loss = 0.02346357\n",
      "Iteration 149, loss = 0.02299431\n",
      "Iteration 150, loss = 0.02283690\n",
      "Iteration 151, loss = 0.02267550\n",
      "Iteration 152, loss = 0.02236107\n",
      "Iteration 153, loss = 0.02218431\n",
      "Iteration 154, loss = 0.02186405\n",
      "Iteration 155, loss = 0.02174535\n",
      "Iteration 156, loss = 0.02165288\n",
      "Iteration 157, loss = 0.02140491\n",
      "Iteration 158, loss = 0.02108980\n",
      "Iteration 159, loss = 0.02079278\n",
      "Iteration 160, loss = 0.02065309\n",
      "Iteration 161, loss = 0.02037571\n",
      "Iteration 162, loss = 0.02024666\n",
      "Iteration 163, loss = 0.01991063\n",
      "Iteration 164, loss = 0.01979822\n",
      "Iteration 165, loss = 0.01967239\n",
      "Iteration 166, loss = 0.01939186\n",
      "Iteration 167, loss = 0.01925676\n",
      "Iteration 168, loss = 0.01901436\n",
      "Iteration 169, loss = 0.01891302\n",
      "Iteration 170, loss = 0.01979994\n",
      "Iteration 171, loss = 0.01869929\n",
      "Iteration 172, loss = 0.01849808\n",
      "Iteration 173, loss = 0.01830655\n",
      "Iteration 174, loss = 0.01809179\n",
      "Iteration 175, loss = 0.01788336\n",
      "Iteration 176, loss = 0.01776032\n",
      "Iteration 177, loss = 0.01750909\n",
      "Iteration 178, loss = 0.01732243\n",
      "Iteration 179, loss = 0.01720556\n",
      "Iteration 180, loss = 0.01718571\n",
      "Iteration 181, loss = 0.01687078\n",
      "Iteration 182, loss = 0.01670995\n",
      "Iteration 183, loss = 0.01671580\n",
      "Iteration 184, loss = 0.01661382\n",
      "Iteration 185, loss = 0.01785042\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "MLPClassifier Confusion matrix: \n",
      "[[5387    3   22   12   11   30   44   20   18   21]\n",
      " [   1 6175   25   11   13   10    7   21   38   10]\n",
      " [  28   31 5140   86   36   19   43   80   61   14]\n",
      " [  17   13   83 5223    7  134    6   39  102   52]\n",
      " [  13   25   42    4 5144   14   40   28   20  152]\n",
      " [  20   10   23  125   10 4665   59   10   86   49]\n",
      " [  47   16   28    4   35   51 5270    4   39    6]\n",
      " [  10   21   64   36   43   15    2 5569   18  103]\n",
      " [  32   39   62  113   22   97   42   24 4942   59]\n",
      " [  31   14   10   44  119   39    5   94   41 5158]]\n",
      "MLPClassifier Classifier accuracy on the training set is 0.9405892857142857\n",
      "MLPClassifier Classifier accuracy on the test set is 0.9457857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('MLPClassifier')\n",
    "mlp = MLPClassifier(solver='sgd', # optimize the loss function\n",
    "                    activation='relu',\n",
    "                    alpha=1e-3,\n",
    "                    hidden_layer_sizes=(80,80),\n",
    "                    random_state=42, # fix the random state, so each run has the same output\n",
    "                    max_iter=1000,\n",
    "                    verbose=1, # 0: do not print out log, >= 1: print out log\n",
    "                    learning_rate_init=.0001)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_train_pred = cross_val_predict(mlp, X_train, y_train, cv=4)\n",
    "\n",
    "# Compute confusion matrix and accuracy score on the training set\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "print('MLPClassifier Confusion matrix: \\n{0}'.format(conf_mx))\n",
    "print('MLPClassifier Classifier accuracy on the training set is {0}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "# Compute accuracy on the test set\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "print('MLPClassifier Classifier accuracy on the test set is {0}'.format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 3 convolutional layers\n",
      "Reshape data\n",
      "test_y_data_trans=[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Start evaluating CNN model by tensorflow...\n",
      "[5, 5, 1, 32] -> <tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "[32] -> <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n",
      "[5, 5, 32, 64] -> <tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "[64] -> <tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\n",
      "[5, 5, 64, 128] -> <tf.Variable 'Variable_4:0' shape=(5, 5, 64, 128) dtype=float32_ref>\n",
      "[128] -> <tf.Variable 'Variable_5:0' shape=(128,) dtype=float32_ref>\n",
      "[2048, 1024] -> <tf.Variable 'Variable_6:0' shape=(2048, 1024) dtype=float32_ref>\n",
      "[1024] -> <tf.Variable 'Variable_7:0' shape=(1024,) dtype=float32_ref>\n",
      "[1024, 10] -> <tf.Variable 'Variable_8:0' shape=(1024, 10) dtype=float32_ref>\n",
      "[10] -> <tf.Variable 'Variable_9:0' shape=(10,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From <ipython-input-7-908be85e136c>:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, SVM 3 convolutional layers training accuracy 0.07999999821186066\n",
      "step 100, SVM 3 convolutional layers training accuracy 0.800000011920929\n",
      "step 200, SVM 3 convolutional layers training accuracy 0.8399999737739563\n",
      "step 300, SVM 3 convolutional layers training accuracy 0.8799999952316284\n",
      "step 400, SVM 3 convolutional layers training accuracy 0.8799999952316284\n",
      "step 500, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 600, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 700, SVM 3 convolutional layers training accuracy 0.9399999976158142\n",
      "step 800, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 900, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 1000, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 1100, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 1200, SVM 3 convolutional layers training accuracy 0.9399999976158142\n",
      "step 1300, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 1400, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 1500, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 1600, SVM 3 convolutional layers training accuracy 0.8999999761581421\n",
      "step 1700, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 1800, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 1900, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 2000, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 2100, SVM 3 convolutional layers training accuracy 0.9399999976158142\n",
      "step 2200, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 2300, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 2400, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 2500, SVM 3 convolutional layers training accuracy 0.9200000166893005\n",
      "step 2600, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 2700, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 2800, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 2900, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3000, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 3100, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3200, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3300, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 3400, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3500, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 3600, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3700, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 3800, SVM 3 convolutional layers training accuracy 0.9599999785423279\n",
      "step 3900, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4000, SVM 3 convolutional layers training accuracy 0.9399999976158142\n",
      "step 4100, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4200, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4300, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4400, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4500, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 4600, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 4700, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4800, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 4900, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5000, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 5100, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5200, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5300, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5400, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5500, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5600, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 5700, SVM 3 convolutional layers training accuracy 0.9800000190734863\n",
      "step 5800, SVM 3 convolutional layers training accuracy 1.0\n",
      "step 5900, SVM 3 convolutional layers training accuracy 1.0\n",
      "SVM 3 convolutional layers test accuracy 0.9838571548461914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "print('SVM 3 convolutional layers')\n",
    "print(\"Reshape data\")\n",
    "train_x_data = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(np.float32) # M*28*28*1, 1 is channel\n",
    "train_y_data = y_train.reshape(y_train.shape[0], 1).astype(np.float32) # M*1, 1 is channel\n",
    "test_x_data = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(np.float32) # (N-M)*28*28*1, 1 is channel\n",
    "test_y_data = y_test.reshape(y_test.shape[0], 1).astype(np.float32) # (N-M)*1, 1 is channel\n",
    "\n",
    "train_x_minmax = train_x_data / 255.0 # each pixel now in [0, 1]\n",
    "test_x_minmax = test_x_data / 255.0\n",
    "\n",
    "# Reformat y into one-hot encoding style\n",
    "# each label become a 10-element vector, such as [1 0 0 0 0 0 0 0 0 0]\n",
    "# At the output of CNN, we can use softmax to do multiclass classification.\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(train_y_data)\n",
    "train_y_data_trans = lb.transform(train_y_data)\n",
    "test_y_data_trans = lb.transform(test_y_data)\n",
    "\n",
    "print('test_y_data_trans={0}'.format(test_y_data_trans))\n",
    "\n",
    "print(\"Start evaluating CNN model by tensorflow...\")\n",
    "\n",
    "# Model input\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1]) # ? * 28*28*1, ? is batch size\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) # ? * 10\n",
    "\n",
    "# Weight initialization\n",
    "def init_weight(weight):\n",
    "    initial = tf.truncated_normal(weight, stddev=0.1)\n",
    "    r = tf.Variable(initial)\n",
    "    print(\"{0} -> {1}\".format(weight, r))\n",
    "    return r\n",
    "\n",
    "def init_bias(bias):\n",
    "    initial = tf.constant(0.1, shape=bias)\n",
    "    r = tf.Variable(initial)\n",
    "    print(\"{0} -> {1}\".format(bias, r))\n",
    "    return r\n",
    "\n",
    "# First convolutional layer\n",
    "# Convolution: compute 32 features for each 5x5 patch\n",
    "# Max pooling: reduce image size to 14x14, because 2*2 -> 1. this is downsample\n",
    "W_conv1 = init_weight([5, 5, 1, 32])\n",
    "b_conv1 = init_bias([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "# `tf.nn.conv2d()` computes a 2-D convolution given 4-D `input` and `filter` tensors\n",
    "# input tensor shape `[batch, in_height, in_width, in_channels]`, batch is number of observation \n",
    "# filter tensor shape `[filter_height, filter_width, in_channels, out_channels]`\n",
    "# strides: the stride of the sliding window for each dimension of input.\n",
    "# padding: 'SAME' or 'VALID', determine the type of padding algorithm to use\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# ksize: the size of the window for each dimension of the input tensor.\n",
    "\n",
    "# Second conv layer\n",
    "# Max pooling: downsample image size to 7x7\n",
    "W_conv2 = init_weight([5, 5, 32, 64])\n",
    "b_conv2 = init_bias([64])\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Third conv layer\n",
    "# Max pooling: downsample image size to 4x4\n",
    "W_conv3 = init_weight([5, 5, 64, 128])\n",
    "b_conv3 = init_bias([128])\n",
    "h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Densely connected layer\n",
    "# Fully-conected layer with 1024 neurons\n",
    "W_fc1 = init_weight([4 * 4 * 128, 1024])\n",
    "b_fc1 = init_bias([1024])\n",
    "\n",
    "h_pool1_flat = tf.reshape(h_pool3, [-1, 4*4*128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout\n",
    "# To reduce overfitting, we apply dropout before the readout layer.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout layer\n",
    "W_fc2 = init_weight([1024, 10])\n",
    "b_fc2 = init_bias([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Train and evaluate\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "# Other solver, optimizer = tf.train.GradientDescentOptimizer(1e-4)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(6000):\n",
    "    # select some records randomly as a batch\n",
    "    sample_index = np.random.choice(train_x_minmax.shape[0], 50)\n",
    "    batch_xs = train_x_minmax[sample_index, :]\n",
    "    batch_ys = train_y_data_trans[sample_index, :]\n",
    "    # print some information during processing\n",
    "    if step % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "        print(\"step {0}, SVM 3 convolutional layers training accuracy {1}\".format(step, train_accuracy))\n",
    "    # Run train a on the batch\n",
    "    sess.run(train, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "print(\"SVM 3 convolutional layers test accuracy {0}\".format(sess.run(accuracy, feed_dict={\n",
    "    x: test_x_minmax, y_: test_y_data_trans, keep_prob: 1.0})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
